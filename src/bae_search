import os, sys, re
import pickle
from time import time
import copy
from dataclasses import dataclass
from typing import Optional

import torch
import torch.nn as nn
import torchvision
import torch.optim as optim
import torch.distributions as dis
import torch.linalg as tla
import numpy as np
from itertools import permutations, combinations
# from tqdm import tqdm
import geoopt as geo

import scipy.stats as sts
import scipy.linalg as la
import scipy.spatial as spt
import scipy.sparse as sprs
import scipy.special as spc
import numpy.linalg as nla
from scipy.optimize import linear_sum_assignment as lsa
from scipy.optimize import linprog as lp
from scipy.optimize import nnls

from numba import njit
import math

# my code
import util

############################################################
######### Jitted update of S ###############################
############################################################

@njit
def binary_glm(X, S, W, b, beta, temp, steps=1, STS=None, N=None, lognorm=bae_util.gaussian):
    """
    One gradient step on S

    lognorm should always be gaussian, the others are all worse for some reason

    TODO: figure out a good sparse implementation?
    """

    n, m = S.shape

    if (STS is None) or (N is None):
        StS = np.dot(S.T, S)
        N = n 
    else:
        StS = 1*STS
        N = 1*N
    St1 = np.diag(StS)

    ## Initial values
    E = np.dot(S,W.T) + b  # Natural parameters
    C = np.dot(X,W)        # Constant term

    for step in range(steps):
        for i in np.random.permutation(np.arange(n)):
        # en = np.zeros((n,m))
        # for i in np.arange(n): 

            if beta > 1e-6:
                ## Organize states
                s = S[i]
                St1 -= s
                StS -= np.outer(s,s)

                ## Regularization (more verbose because of numba reasons)
                D1 = StS
                D2 = St1[None,:] - StS
                D3 = St1[:,None] - StS
                D4 = (N-1) - St1[None,:] - St1[:,None] + StS

                best1 = 1*(D1<D2)*(D1<D3)*(D1<D4)
                best2 = 1*(D2<D1)*(D2<D3)*(D2<D4)
                best3 = 1*(D3<D2)*(D3<D1)*(D3<D4)
                best4 = 1*(D4<D2)*(D4<D3)*(D4<D1)

                R = (best1 - best2 - best3 + best4)*1.0
                r = (best2.sum(0) - best4.sum(0))*1.0

            ## Hopfield update of s
            for j in np.random.permutation(np.arange(m)): # concept

                ## Compute linear terms
                dot = np.sum(lognorm(E[i] + (1-S[i,j])*W[:,j])) 
                dot -= np.sum(lognorm(E[i] - S[i,j]*W[:,j]))
                if beta > 1e-6:
                    inhib = np.dot(R[j], S[i]) + r[j]
                else:
                    inhib = 0

                ## Compute currents
                curr = (C[i,j] - beta*inhib - dot)/temp

                ## Apply sigmoid (overflow robust)
                if curr < -100:
                    prob = 0.0
                elif curr > 100:
                    prob = 1.0
                else:
                    prob = 1.0 / (1.0 + math.exp(-curr))

                ## Update outputs
                sj = 1*(np.random.rand() < prob)
                ds = sj - S[i,j]
                S[i,j] = sj
                
                ## Update dot products
                E[i] += ds*W[:,j]

                # en[i,j] = np.sum(lognorm(E)) - 2*np.sum(C*S)

            ## Update 
            # S[i] = news
            St1 += S[i]
            StS += np.outer(S[i], S[i]) 

    return S #, en

@njit
def update_concepts_asym(X, S, W, scl, beta, temp, STS=None, N=None, steps=1):
    """
    One gradient step on S

    TODO: figure out a good sparse implementation!
    """

    n, m = S.shape

    if (STS is None) or (N is None):
        StS = np.dot(S.T, S)
        N = n 
    else:
        StS = 1*STS
        N = 1*N
    St1 = np.diag(StS)

    ## Initial values
    C = np.dot(X,W)        # Constant term

    for step in range(steps):
        for i in np.random.permutation(np.arange(n)):
        # en = np.zeros((n,m))
        # for i in np.arange(n): 

            ## Organize states
            St1 -= S[i]
            StS -= np.outer(S[i],S[i])

            ## Regularization (more verbose because of numba reasons)
            D1 = StS
            D2 = St1[None,:] - StS
            D3 = St1[:,None] - StS
            D4 = (N-1) - St1[None,:] - St1[:,None] + StS

            best1 = 1*(D1<D2)*(D1<D3)*(D1<D4)
            best2 = 1*(D2<D1)*(D2<D3)*(D2<D4)
            best3 = 1*(D3<D2)*(D3<D1)*(D3<D4)
            best4 = 1*(D4<D2)*(D4<D3)*(D4<D1)

            R = (best1 - best2 - best3 + best4)*1.0
            r = (best2.sum(0) - best4.sum(0))*1.0

            ## Hopfield update of s
            for j in np.random.permutation(np.arange(m)): # concept

                ## Compute linear terms
                inhib = np.dot(R[j], S[i]) + r[j]

                ## Compute currents
                curr = (2*C[i,j] - beta*scl*inhib - scl)/temp

                # ## Apply sigmoid (overflow robust)
                if curr < -100:
                    prob = 0.0
                elif curr > 100:
                    prob = 1.0
                else:
                    prob = 1.0 / (1.0 + math.exp(-curr))

                ## Update outputs
                sj = 1*(np.random.rand() < prob)
                ds = sj - S[i,j]
                S[i,j] = sj

                # en[i,j] = np.sum(lognorm(E)) - 2*np.sum(C*S)

            ## Update 
            # S[i] = news
            St1 += S[i]
            StS += np.outer(S[i], S[i]) 

    return S #, en

@njit
def update_concepts_asym_cntr(X, S, W, scl, alpha, beta, temp, STS=None, N=None, steps=1):
    """
    One gradient step on S

    TODO: figure out a good sparse implementation!
    """

    n, m = S.shape

    if (STS is None) or (N is None):
        StS = np.dot(S.T, S)
        N = n
    else:
        StS = 1*STS
        N = 1*N
    St1 = np.diag(StS)

    ## Initial values
    C = np.dot(X,W)        # Constant term
    
    for step in range(steps):
        for i in np.random.permutation(np.arange(n)):

            ## Organize states
            St1 -= S[i]

            if beta >= 1e-6:
                StS -= np.outer(S[i], S[i])

                ## Regularization (more verbose because of numba reasons)
                D1 = StS
                D2 = St1[None,:] - StS
                D3 = St1[:,None] - StS
                D4 = (N-1) - St1[None,:] - St1[:,None] + StS

                best1 = 1*(D1<D2)*(D1<D3)*(D1<D4)
                best2 = 1*(D2<D1)*(D2<D3)*(D2<D4)
                best3 = 1*(D3<D2)*(D3<D1)*(D3<D4)
                best4 = 1*(D4<D2)*(D4<D3)*(D4<D1)

                R = (best1 - best2 - best3 + best4)*1.0
                r = (best2.sum(0) - best4.sum(0))*1.0

            ## Hopfield update of s
            for j in np.random.permutation(np.arange(m)): # concept

                inp = (2*C[i,j] - scl*(N - 1 - 2*St1[j])/N - alpha)
                # inp = (2*np.dot(X[i], W[:,j]) - scl*(n - 1 - 2*St1[j])/n - alpha)

                ## Compute linear terms
                if beta >= 1e-6:
                    inhib = np.dot(R[j], S[i]) + r[j]
                else:
                    inhib = 0

                ## Compute currents
                curr = (inp - beta*scl*inhib)/temp

                # ## Apply sigmoid (overflow robust)
                if curr < -100:
                    prob = 0.0
                elif curr > 100:
                    prob = 1.0
                else:
                    prob = 1.0 / (1.0 + math.exp(-curr))

                ## Update outputs
                S[i,j] = 1*(np.random.rand() < prob)

            ## Update 
            # S[i] = news
            St1 += S[i]
            StS += np.outer(S[i], S[i]) 

    return S #, en

@njit(cache=True)
def update_concepts_kernel(X: np.ndarray, 
                           S: np.ndarray, 
                           scl: float, 
                           beta: float, 
                           temp: float, 
                           STS: Optional[np.ndarray]=None,
                           STX: Optional[np.ndarray]=None, 
                           N: Optional[int]=None, 
                           steps: Optional[int]=1) -> np.ndarray:
    """
    One batch gradient step on S

    Assumes that X is centered *with respect to the full dataset* and that, 
    if supplied, StS and StX are also computed for the full dataset. 

    TODO: figure out a good sparse implementation?
    """

    n, m = S.shape

    if (STS is None) or (STX is None) or (N is None):
        StS = np.dot(S.T, S)
        StX = np.dot(S.T, X)
        N = n
    else:
        StS = 1*STS 
        StX = 1*STX
        N = 1*N

    St1 = np.diag(StS)

    for i in np.random.permutation(np.arange(n)):
    # for i in np.arange(n):

        ## Pick current item
        t = (N-1)/N

        x = X[i]
        s = S[i]

        ## Subtract current item
        St1 -= s
        StS -= np.outer(s,s)
        StX -= np.outer(s,x)

        ## Organize states
        xtx = np.sum(x**2)
        Sk = (np.dot(StX, x) + St1*xtx/(N-1))/t
        k0 = xtx/(t**2)

        ## Regularization (more verbose because of numba reasons)
        D1 = StS
        D2 = St1[None,:] - StS
        D3 = St1[:,None] - StS
        D4 = (N-1) - St1[None,:] - St1[:,None] + StS

        best1 = 1*(D1<D2)*(D1<D3)*(D1<D4)
        best2 = 1*(D2<D1)*(D2<D3)*(D2<D4)
        best3 = 1*(D3<D2)*(D3<D1)*(D3<D4)
        best4 = 1*(D4<D2)*(D4<D3)*(D4<D1)

        R = (best1 - best2 - best3 + best4)*1.0
        r = (best2.sum(0) - best4.sum(0))*1.0
        
        ## Recurrence
        # Compute the rank-one terms
        s_ = St1/(N-1)
        u = 2*s_ - 1

        s_sc_ = s_*(1-s_)

        ## Constants
        # sx = Sx.sum()
        sx = np.dot(s_, s - s_)
        ux = 2*sx - s.sum() + s_.sum()
        
        # Form the threshold 
        h = t*((scl**2)*s_sc_.sum() - scl*k0)*u + 2*scl*Sk - beta*(scl**2)*r
        
        # Need to subtract the diagonal and add it back in
        Jii = 2*(N-1)*s_sc_ + t*u**2

        ## Hopfield update of s
        news = 1*s
        for step in range(steps):
            for j in np.random.permutation(np.arange(m)):
            # for j in range(m): # concept

                # rows = ridx[rptr[j]:rptr[j+1]]

                # Compute sparse dot product
                dot = 2*np.dot(StS[j], news - s_)
                dot -= 2*(N-1)*s_[j]*sx
                dot += t*u[j]*ux
                dot -= Jii[j]*news[j]
                dot += beta*np.dot(R[j], news)

                ## Compute currents
                curr = (h[j] - (scl**2)*Jii[j]/2 - (scl**2)*dot)/temp

                ## Apply sigmoid (overflow robust)
                if curr < -100:
                    prob = 0.0
                elif curr > 100:
                    prob = 1.0
                else:
                    prob = 1.0 / (1.0 + math.exp(-curr))
                
                ## Update outputs
                sj = 1*(np.random.rand() < prob)
                ds = sj - news[j]
                news[j] = sj
                
                ## Update dot products
                if np.abs(ds) > 0:
                    sx += ds*s_[j]
                    ux += ds*u[j]

        ## Update 
        S[i] = news
        St1 += news
        StS += np.outer(news, news)
        StX += np.outer(news, x)

        ## Keep in sparsest form
        # flip = (St1 > (n//2))

        # StS[flip] = St1[None,:] - StS[flip]
        # St1[flip] = n - St1[flip]
        # StS[:,flip] = St1[:,None] - StS[:,flip]
    
    return S
